{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import sys\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import rich\n",
    "from rich import print as rprint\n",
    "import os\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"embedder\": \"intfloat/multilingual-e5-large-instruct\",\n",
    "    \"embedding_dimension\": 1024,\n",
    "    \"use_description\": True,\n",
    "    \"use_instruct\": False,\n",
    "}\n",
    "\n",
    "EMBEDDINGS_DIMENSION = CONFIG[\"embedding_dimension\"]\n",
    "USE_DESCRIPTION = CONFIG[\"use_description\"]\n",
    "USE_INSTRUCT = CONFIG[\"use_instruct\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37652/2076140095.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_classifier.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassificationHead(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): GELU(approximate='none')\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=27, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        # Input dimension depends on whether we're using description\n",
    "        combined_dim = input_dim * 2 if USE_DESCRIPTION else input_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x1, x2=None):\n",
    "        if USE_DESCRIPTION:\n",
    "            combined = torch.cat((x1, x2), dim=1)\n",
    "        else:\n",
    "            combined = x1\n",
    "        return self.classifier(combined)\n",
    "\n",
    "\n",
    "# load model from path\n",
    "path = \"models/intfloat/multilingual-e5-large-instruct-0.86f1.pt\"\n",
    "\n",
    "text_classifier = ClassificationHead(EMBEDDINGS_DIMENSION, 27)\n",
    "text_classifier.load_state_dict(torch.load(path))\n",
    "text_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_path(texts, embedder_name, use_instruct):\n",
    "    \"\"\"Generate a unique cache path based on input texts, embedder, and instruct setting\"\"\"\n",
    "    text_hash = hashlib.md5(''.join(texts).encode()).hexdigest()\n",
    "    embedder_hash = hashlib.md5(embedder_name.encode()).hexdigest()\n",
    "    instruct_suffix = '_instruct' if use_instruct else ''\n",
    "    return f'cache/embeddings_{embedder_hash}_{text_hash}{instruct_suffix}.pt'\n",
    "# Function to get embeddings in batches\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer(CONFIG[\"embedder\"], trust_remote_code=True).to('cuda')\n",
    "model.train()\n",
    "\n",
    "def get_embeddings(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Getting embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        # Only apply instruct formatting if USE_INSTRUCT is True\n",
    "        processed_batch = [\n",
    "            get_detailed_instruct(\"Match similar products based on their features, characteristics, and intended use.\", text) \n",
    "            if USE_INSTRUCT and text.strip()\n",
    "            else text \n",
    "            for text in batch\n",
    "        ]\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model.encode(processed_batch, max_length=EMBEDDINGS_DIMENSION)\n",
    "            if isinstance(batch_embeddings, np.ndarray):\n",
    "                batch_embeddings = torch.from_numpy(batch_embeddings)\n",
    "            assert batch_embeddings.shape[1] == EMBEDDINGS_DIMENSION, f\"Model output dimension mismatch. Expected {EMBEDDINGS_DIMENSION}, got {batch_embeddings.shape[1]}\"\n",
    "            embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "def load_or_compute_embeddings(texts, embedder_name, batch_size=32):\n",
    "    \"\"\"Load embeddings from cache if they exist, otherwise compute and cache them\"\"\"\n",
    "    # Create cache directory if it doesn't exist\n",
    "    os.makedirs('cache', exist_ok=True)\n",
    "    \n",
    "    cache_path = get_cache_path(texts, embedder_name, USE_INSTRUCT)\n",
    "    \n",
    "    # Try to load from cache\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading embeddings from cache: {cache_path}\")\n",
    "        return torch.load(cache_path)\n",
    "    \n",
    "    # Compute embeddings\n",
    "    print(\"Computing new embeddings...\")\n",
    "    embeddings = get_embeddings(texts, batch_size)\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"Saving embeddings to cache: {cache_path}\")\n",
    "    torch.save(embeddings, cache_path)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing new embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 432/432 [00:13<00:00, 32.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to cache: cache/embeddings_571f3efdd580e6d678ec91a1b96b1ca1_ff21f37c2835453e2ef54e19667bb025.pt\n",
      "Computing new embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 432/432 [02:05<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to cache: cache/embeddings_571f3efdd580e6d678ec91a1b96b1ca1_7247ebafd4fcd6572425f0d8704a9543.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom dataset class\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, designation_embeddings, description_embeddings, labels):\n",
    "        assert designation_embeddings.shape[1] == EMBEDDINGS_DIMENSION, f\"Designation embeddings dimension mismatch. Expected {EMBEDDINGS_DIMENSION}, got {designation_embeddings.shape[1]}\"\n",
    "        if USE_DESCRIPTION:\n",
    "            assert description_embeddings.shape[1] == EMBEDDINGS_DIMENSION, f\"Description embeddings dimension mismatch. Expected {EMBEDDINGS_DIMENSION}, got {description_embeddings.shape[1]}\"\n",
    "        self.designation_embeddings = designation_embeddings\n",
    "        self.description_embeddings = description_embeddings if USE_DESCRIPTION else None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.designation_embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if USE_DESCRIPTION:\n",
    "            return (self.designation_embeddings[idx], \n",
    "                    self.description_embeddings[idx], \n",
    "                    self.labels[idx])\n",
    "        return (self.designation_embeddings[idx], self.labels[idx])\n",
    "\n",
    "\n",
    "df = pd.read_csv('X_test.csv')\n",
    "designations = [preprocess(text) for text in df['designation'].tolist()]\n",
    "descriptions = [preprocess(text) for text in df['description'].tolist()]\n",
    "\n",
    "labels = [0] * len(designations)\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = i\n",
    "\n",
    "designations_embeddings = load_or_compute_embeddings(designations, CONFIG[\"embedder\"])\n",
    "descriptions_embeddings = load_or_compute_embeddings(descriptions, CONFIG[\"embedder\"])\n",
    "\n",
    "dataset = TextClassificationDataset(designations_embeddings, descriptions_embeddings, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13812, 27])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        if USE_DESCRIPTION:\n",
    "            batch_des, batch_desc, batch_labels = batch\n",
    "            batch_desc = batch_desc.to(device)\n",
    "        else:\n",
    "            batch_des, batch_labels = batch\n",
    "            batch_desc = None\n",
    "        \n",
    "        batch_des = batch_des.to(device)\n",
    "        outputs = text_classifier(batch_des, batch_desc)\n",
    "        # apply softmax\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        all_outputs.append(outputs)\n",
    "\n",
    "all_outputs = torch.cat(all_outputs, dim=0)\n",
    "print(all_outputs.shape)\n",
    "\n",
    "# save to pt\n",
    "torch.save(all_outputs, 'text_softmaxes_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global model and processor cache\n",
    "_model = None\n",
    "_processor = None\n",
    "\n",
    "\n",
    "def get_model_and_processor():\n",
    "    \"\"\"Cache and return the model and processor\"\"\"\n",
    "    global _model, _processor\n",
    "    if _model is None or _processor is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        _model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "        _processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Optimize model with torch.compile() if using PyTorch 2.0+\n",
    "        if hasattr(torch, 'compile'):\n",
    "            _model = torch.compile(_model)\n",
    "        \n",
    "        _model.eval()  # Set model to evaluation mode\n",
    "    return _model, _processor\n",
    "\n",
    "\n",
    "\n",
    "def process_batch(image_paths: List[str], categories: List[str], batch_size: int = 32) -> List[List[Tuple[str, float]]]:\n",
    "    \"\"\"Process images in batches for better performance\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, processor = get_model_and_processor()\n",
    "    \n",
    "    all_similarities = []\n",
    "    \n",
    "    # Process images in batches\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        # Load images\n",
    "        for idx, path in enumerate(batch_paths):\n",
    "            try:\n",
    "                image = Image.open(path)\n",
    "                images.append(image)\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {path}: {e}\")\n",
    "                all_similarities.append(None)\n",
    "        \n",
    "        if not images:\n",
    "            continue\n",
    "            \n",
    "        # Process batch\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(\n",
    "                images=images,\n",
    "                text=[f\"a photo of {category}\" for category in categories],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            similarities = outputs.logits_per_image.softmax(dim=-1)\n",
    "            all_similarities.append(similarities)\n",
    "    \n",
    "    return all_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13812\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"X_test.csv\")\n",
    "\n",
    "\n",
    "all_paths = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    path = f\"images/image_test/image_{getattr(row, 'imageid')}_product_{getattr(row, 'productid')}.jpg\"\n",
    "    all_paths.append(path)\n",
    "\n",
    "\n",
    "print(len(all_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_ids = {\n",
    "        \"10\": \"Single book\",\n",
    "        \"40\": \"Video Game Covers\",\n",
    "        \"50\": \"Game Accessories\",\n",
    "        \"60\": \"Game Console\",\n",
    "        \"1140\": \"Video Game Figurines\",\n",
    "        \"1160\": \"Cards\",\n",
    "        \"1180\": \"Movies Figurine\",\n",
    "        \"1280\": \"Plush toy\",\n",
    "        \"1281\": \"Tabletop Game\",\n",
    "        \"1300\": \"Toy Car\",\n",
    "        \"1301\": \"Game Room Accessories\",\n",
    "        \"1302\": \"Outdoor Toys\",\n",
    "        \"1320\": \"Baby accessories\",\n",
    "        \"1560\": \"Furnitures\",\n",
    "        \"1920\": \"Pillows\",\n",
    "        \"1940\": \"Food & Beverages\",\n",
    "        \"2060\": \"Flags and decorations\",\n",
    "        \"2220\": \"Pet Supplies\",\n",
    "        \"2280\": \"Journals\",\n",
    "        \"2403\": \"Book collection\",\n",
    "        \"2462\": \"Game Console (occasion)\",\n",
    "        \"2522\": \"Office Supplies\",\n",
    "        \"2582\": \"Outdoor & Garden\",\n",
    "        \"2583\": \"Spa Supplies\",\n",
    "        \"2585\": \"Tools & Home Improvement\",\n",
    "        \"2705\": \"Literature book\",\n",
    "        \"2905\": \"PC games\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "432\n"
     ]
    }
   ],
   "source": [
    "res = process_batch(all_paths, category_ids.values(), 32)\n",
    "print(type(res))\n",
    "print(len(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13812, 27)\n"
     ]
    }
   ],
   "source": [
    "full_res = torch.tensor([])\n",
    "\n",
    "for tensor in res:\n",
    "    full_res = torch.cat((full_res, tensor.to(\"cpu\")), dim=0)\n",
    "\n",
    "full_res = full_res.cpu().numpy()\n",
    "print(full_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13812, 27])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to pt\n",
    "full_res = torch.tensor(full_res)\n",
    "\n",
    "# save as pt\n",
    "torch.save(full_res, \"image_softmaxes_test.pt\")\n",
    "full_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          10\n",
       "1        2280\n",
       "2          50\n",
       "3        1280\n",
       "4        2705\n",
       "         ... \n",
       "84911      40\n",
       "84912    2583\n",
       "84913    2280\n",
       "84914    1560\n",
       "84915    2522\n",
       "Name: class, Length: 84916, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save labels\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "labels = df['class']\n",
    "\n",
    "labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Custom Dataset\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, text_data_path, image_data_path):\n",
    "        self.text_data = torch.load(text_data_path).to(device)\n",
    "        self.image_data = torch.load(image_data_path).to(device)\n",
    "        assert len(self.text_data) == len(self.image_data), \"Datasets must have same length\"\n",
    "        df = pd.read_csv(\"output_X_train_update.csv\")\n",
    "        self.le = LabelEncoder()\n",
    "        encoded_labels = self.le.fit_transform(df[\"class\"].values)\n",
    "        self.labels = torch.tensor(encoded_labels, dtype=torch.long).to(device)\n",
    "        print(len(self.labels), self.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_tensor = self.text_data[idx]  # Already on device\n",
    "        image_tensor = self.image_data[idx]  # Already on device\n",
    "        combined_input = torch.mean(torch.stack([0.7*text_tensor, 0.3*image_tensor]), dim=0)\n",
    "        label = self.labels[idx]\n",
    "        return combined_input, label\n",
    "\n",
    "# MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=54):  # 27 + 27 = 54 input features\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 27)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=device):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = val_loss/len(val_loader)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "        \n",
    "        # Create a nice formatted output with tqdm\n",
    "        from tqdm.auto import tqdm\n",
    "        \n",
    "        # Update progress bar description with metrics\n",
    "        desc = (f'Epoch {epoch+1}/{num_epochs} | '\n",
    "               f'Train Loss: {train_loss/len(train_loader):.4f} | '\n",
    "               f'Val Loss: {avg_val_loss:.4f} | '\n",
    "               f'Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        # Use tqdm.write to avoid interfering with progress bars\n",
    "        tqdm.write(\"\\n\" + \"=\" * 80)\n",
    "        tqdm.write(desc)\n",
    "        tqdm.write(\"=\" * 80)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            tqdm.write(f'\\n🔥 New best validation loss: {avg_val_loss:.4f}! Saving model...\\n')\n",
    "            torch.save(model.state_dict(), 'late_merger.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                tqdm.write(f'\\nEarly stopping triggered after {epoch+1} epochs - no improvement in validation loss for {patience} epochs\\n')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43727/4053801610.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.text_data = torch.load(text_data_path).to(device)\n",
      "/tmp/ipykernel_43727/4053801610.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.image_data = torch.load(image_data_path).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84916 tensor([ 0, 18,  2,  ..., 18, 13, 21], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 63\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     61\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     62\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 63\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# optimizer.step()\u001b[39;00m\n\u001b[1;32m     66\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/RakutenMultimodalClassification/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RakutenMultimodalClassification/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RakutenMultimodalClassification/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Main execution\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "# Load dataset\n",
    "dataset = CombinedDataset('text_softmaxes.pt', 'image_softmaxes.pt')\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "# take only 1% of each \n",
    "train_dataset = train_dataset[:int(len(train_dataset)*0.01)]\n",
    "val_dataset = val_dataset[:int(len(val_dataset)*0.01)]\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train model\n",
    "train_model(model, train_loader, val_loader, criterion, None, NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[2.7354e-04, 7.9283e-08, 3.4605e-08,  ..., 5.2863e-04, 3.8494e-01,\n",
      "         1.7149e-04],\n",
      "        [2.6582e-07, 1.7240e-08, 5.0679e-08,  ..., 3.5986e-02, 8.9124e-04,\n",
      "         2.3434e-02],\n",
      "        [1.1106e-07, 5.6565e-08, 9.1660e-08,  ..., 2.9585e-04, 5.0665e-02,\n",
      "         2.9756e-01],\n",
      "        ...,\n",
      "        [1.5287e-05, 1.2892e-04, 6.5116e-06,  ..., 8.1616e-03, 1.7940e-02,\n",
      "         3.4464e-02],\n",
      "        [7.8493e-11, 2.2006e-09, 7.6715e-10,  ..., 5.7081e-04, 1.2668e-04,\n",
      "         1.5415e-03],\n",
      "        [1.4320e-08, 2.9513e-08, 4.9837e-08,  ..., 7.0822e-03, 4.5104e-03,\n",
      "         1.4598e-03]], device='cuda:0'), tensor([19, 16, 15, 26,  5, 18,  8,  3,  7, 13, 16,  5, 16, 11, 11, 16, 23,  8,\n",
      "        11, 21,  5,  4,  7,  0, 23,  2, 14,  7,  7,  9, 17, 15],\n",
      "       device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "# show 5 vlaues of trainloader\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43727/1180809839.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  late_merger.load_state_dict(torch.load('late_merger.pt'))\n",
      "/tmp/ipykernel_43727/3313776226.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.text_data = torch.load(text_data_path).to(device)\n",
      "/tmp/ipykernel_43727/3313776226.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.image_data = torch.load(image_data_path).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84916 tensor([ 0, 18,  2,  ..., 18, 13, 21], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "late_merger = MLP()\n",
    "late_merger.load_state_dict(torch.load('late_merger.pt'))\n",
    "\n",
    "dataset = CombinedDataset(\"text_softmaxes_test.pt\", \"image_softmaxes_test.pt\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "late_merger.eval()\n",
    "print(device)\n",
    "late_merger.to(device)\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels  in dataloader:\n",
    "        inputs, _ = inputs.to(device), labels.to(device)\n",
    "        best = torch.argmax(inputs, dim=1)\n",
    "        all_preds.extend(best.cpu().numpy())\n",
    "\n",
    "len(all_preds)\n",
    "\n",
    "real_preds = dataset.le.inverse_transform(all_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('X_test.csv')\n",
    "res_df = pd.DataFrame({\"\": df_test['Unnamed: 0'], 'prdtypecode': real_preds})\n",
    "res_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
