{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Desktop/RakutenMultimodalClassification/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import sys\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import rich\n",
    "from rich import print as rprint\n",
    "import os\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"embedder\": \"intfloat/multilingual-e5-large-instruct\",\n",
    "    \"embedding_dimension\": 1024,\n",
    "    \"use_description\": True,\n",
    "    \"use_instruct\": False,\n",
    "}\n",
    "\n",
    "EMBEDDINGS_DIMENSION = CONFIG[\"embedding_dimension\"]\n",
    "USE_DESCRIPTION = CONFIG[\"use_description\"]\n",
    "USE_INSTRUCT = CONFIG[\"use_instruct\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37652/2076140095.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_classifier.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassificationHead(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): GELU(approximate='none')\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=27, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        # Input dimension depends on whether we're using description\n",
    "        combined_dim = input_dim * 2 if USE_DESCRIPTION else input_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x1, x2=None):\n",
    "        if USE_DESCRIPTION:\n",
    "            combined = torch.cat((x1, x2), dim=1)\n",
    "        else:\n",
    "            combined = x1\n",
    "        return self.classifier(combined)\n",
    "\n",
    "\n",
    "# load model from path\n",
    "path = \"models/intfloat/multilingual-e5-large-instruct-0.86f1.pt\"\n",
    "\n",
    "text_classifier = ClassificationHead(EMBEDDINGS_DIMENSION, 27)\n",
    "text_classifier.load_state_dict(torch.load(path))\n",
    "text_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_path(texts, embedder_name, use_instruct):\n",
    "    \"\"\"Generate a unique cache path based on input texts, embedder, and instruct setting\"\"\"\n",
    "    text_hash = hashlib.md5(''.join(texts).encode()).hexdigest()\n",
    "    embedder_hash = hashlib.md5(embedder_name.encode()).hexdigest()\n",
    "    instruct_suffix = '_instruct' if use_instruct else ''\n",
    "    return f'cache/embeddings_{embedder_hash}_{text_hash}{instruct_suffix}.pt'\n",
    "# Function to get embeddings in batches\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer(CONFIG[\"embedder\"], trust_remote_code=True).to('cuda')\n",
    "model.train()\n",
    "\n",
    "def get_embeddings(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Getting embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        # Only apply instruct formatting if USE_INSTRUCT is True\n",
    "        processed_batch = [\n",
    "            get_detailed_instruct(\"Match similar products based on their features, characteristics, and intended use.\", text) \n",
    "            if USE_INSTRUCT and text.strip()\n",
    "            else text \n",
    "            for text in batch\n",
    "        ]\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model.encode(processed_batch, max_length=EMBEDDINGS_DIMENSION)\n",
    "            if isinstance(batch_embeddings, np.ndarray):\n",
    "                batch_embeddings = torch.from_numpy(batch_embeddings)\n",
    "            assert batch_embeddings.shape[1] == EMBEDDINGS_DIMENSION, f\"Model output dimension mismatch. Expected {EMBEDDINGS_DIMENSION}, got {batch_embeddings.shape[1]}\"\n",
    "            embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "def load_or_compute_embeddings(texts, embedder_name, batch_size=32):\n",
    "    \"\"\"Load embeddings from cache if they exist, otherwise compute and cache them\"\"\"\n",
    "    # Create cache directory if it doesn't exist\n",
    "    os.makedirs('cache', exist_ok=True)\n",
    "    \n",
    "    cache_path = get_cache_path(texts, embedder_name, USE_INSTRUCT)\n",
    "    \n",
    "    # Try to load from cache\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading embeddings from cache: {cache_path}\")\n",
    "        return torch.load(cache_path)\n",
    "    \n",
    "    # Compute embeddings\n",
    "    print(\"Computing new embeddings...\")\n",
    "    embeddings = get_embeddings(texts, batch_size)\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"Saving embeddings to cache: {cache_path}\")\n",
    "    torch.save(embeddings, cache_path)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing new embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 432/432 [00:13<00:00, 32.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to cache: cache/embeddings_571f3efdd580e6d678ec91a1b96b1ca1_ff21f37c2835453e2ef54e19667bb025.pt\n",
      "Computing new embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 432/432 [02:05<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to cache: cache/embeddings_571f3efdd580e6d678ec91a1b96b1ca1_7247ebafd4fcd6572425f0d8704a9543.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom dataset class\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, designation_embeddings, description_embeddings, labels):\n",
    "        assert designation_embeddings.shape[1] == EMBEDDINGS_DIMENSION, f\"Designation embeddings dimension mismatch. Expected {EMBEDDINGS_DIMENSION}, got {designation_embeddings.shape[1]}\"\n",
    "        if USE_DESCRIPTION:\n",
    "            assert description_embeddings.shape[1] == EMBEDDINGS_DIMENSION, f\"Description embeddings dimension mismatch. Expected {EMBEDDINGS_DIMENSION}, got {description_embeddings.shape[1]}\"\n",
    "        self.designation_embeddings = designation_embeddings\n",
    "        self.description_embeddings = description_embeddings if USE_DESCRIPTION else None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.designation_embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if USE_DESCRIPTION:\n",
    "            return (self.designation_embeddings[idx], \n",
    "                    self.description_embeddings[idx], \n",
    "                    self.labels[idx])\n",
    "        return (self.designation_embeddings[idx], self.labels[idx])\n",
    "\n",
    "\n",
    "df = pd.read_csv('X_test.csv')\n",
    "designations = [preprocess(text) for text in df['designation'].tolist()]\n",
    "descriptions = [preprocess(text) for text in df['description'].tolist()]\n",
    "\n",
    "labels = [0] * len(designations)\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = i\n",
    "\n",
    "designations_embeddings = load_or_compute_embeddings(designations, CONFIG[\"embedder\"])\n",
    "descriptions_embeddings = load_or_compute_embeddings(descriptions, CONFIG[\"embedder\"])\n",
    "\n",
    "dataset = TextClassificationDataset(designations_embeddings, descriptions_embeddings, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13812, 27])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        if USE_DESCRIPTION:\n",
    "            batch_des, batch_desc, batch_labels = batch\n",
    "            batch_desc = batch_desc.to(device)\n",
    "        else:\n",
    "            batch_des, batch_labels = batch\n",
    "            batch_desc = None\n",
    "        \n",
    "        batch_des = batch_des.to(device)\n",
    "        outputs = text_classifier(batch_des, batch_desc)\n",
    "        # apply softmax\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        all_outputs.append(outputs)\n",
    "\n",
    "all_outputs = torch.cat(all_outputs, dim=0)\n",
    "print(all_outputs.shape)\n",
    "\n",
    "# save to pt\n",
    "torch.save(all_outputs, 'text_softmaxes_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global model and processor cache\n",
    "_model = None\n",
    "_processor = None\n",
    "\n",
    "\n",
    "def get_model_and_processor():\n",
    "    \"\"\"Cache and return the model and processor\"\"\"\n",
    "    global _model, _processor\n",
    "    if _model is None or _processor is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        _model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "        _processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Optimize model with torch.compile() if using PyTorch 2.0+\n",
    "        if hasattr(torch, 'compile'):\n",
    "            _model = torch.compile(_model)\n",
    "        \n",
    "        _model.eval()  # Set model to evaluation mode\n",
    "    return _model, _processor\n",
    "\n",
    "\n",
    "\n",
    "def process_batch(image_paths: List[str], categories: List[str], batch_size: int = 32) -> List[List[Tuple[str, float]]]:\n",
    "    \"\"\"Process images in batches for better performance\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, processor = get_model_and_processor()\n",
    "    \n",
    "    all_similarities = []\n",
    "    \n",
    "    # Process images in batches\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        # Load images\n",
    "        for idx, path in enumerate(batch_paths):\n",
    "            try:\n",
    "                image = Image.open(path)\n",
    "                images.append(image)\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {path}: {e}\")\n",
    "                all_similarities.append(None)\n",
    "        \n",
    "        if not images:\n",
    "            continue\n",
    "            \n",
    "        # Process batch\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(\n",
    "                images=images,\n",
    "                text=[f\"a photo of {category}\" for category in categories],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            similarities = outputs.logits_per_image.softmax(dim=-1)\n",
    "            all_similarities.append(similarities)\n",
    "    \n",
    "    return all_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13812\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"X_test.csv\")\n",
    "\n",
    "\n",
    "all_paths = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    path = f\"images/image_test/image_{getattr(row, 'imageid')}_product_{getattr(row, 'productid')}.jpg\"\n",
    "    all_paths.append(path)\n",
    "\n",
    "\n",
    "print(len(all_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_ids = {\n",
    "        \"10\": \"Single book\",\n",
    "        \"40\": \"Video Game Covers\",\n",
    "        \"50\": \"Game Accessories\",\n",
    "        \"60\": \"Game Console\",\n",
    "        \"1140\": \"Video Game Figurines\",\n",
    "        \"1160\": \"Cards\",\n",
    "        \"1180\": \"Movies Figurine\",\n",
    "        \"1280\": \"Plush toy\",\n",
    "        \"1281\": \"Tabletop Game\",\n",
    "        \"1300\": \"Toy Car\",\n",
    "        \"1301\": \"Game Room Accessories\",\n",
    "        \"1302\": \"Outdoor Toys\",\n",
    "        \"1320\": \"Baby accessories\",\n",
    "        \"1560\": \"Furnitures\",\n",
    "        \"1920\": \"Pillows\",\n",
    "        \"1940\": \"Food & Beverages\",\n",
    "        \"2060\": \"Flags and decorations\",\n",
    "        \"2220\": \"Pet Supplies\",\n",
    "        \"2280\": \"Journals\",\n",
    "        \"2403\": \"Book collection\",\n",
    "        \"2462\": \"Game Console (occasion)\",\n",
    "        \"2522\": \"Office Supplies\",\n",
    "        \"2582\": \"Outdoor & Garden\",\n",
    "        \"2583\": \"Spa Supplies\",\n",
    "        \"2585\": \"Tools & Home Improvement\",\n",
    "        \"2705\": \"Literature book\",\n",
    "        \"2905\": \"PC games\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "432\n"
     ]
    }
   ],
   "source": [
    "res = process_batch(all_paths, category_ids.values(), 32)\n",
    "print(type(res))\n",
    "print(len(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13812, 27)\n"
     ]
    }
   ],
   "source": [
    "full_res = torch.tensor([])\n",
    "\n",
    "for tensor in res:\n",
    "    full_res = torch.cat((full_res, tensor.to(\"cpu\")), dim=0)\n",
    "\n",
    "full_res = full_res.cpu().numpy()\n",
    "print(full_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13812, 27])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to pt\n",
    "full_res = torch.tensor(full_res)\n",
    "\n",
    "# save as pt\n",
    "torch.save(full_res, \"image_softmaxes_test.pt\")\n",
    "full_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          10\n",
       "1        2280\n",
       "2          50\n",
       "3        1280\n",
       "4        2705\n",
       "         ... \n",
       "84911      40\n",
       "84912    2583\n",
       "84913    2280\n",
       "84914    1560\n",
       "84915    2522\n",
       "Name: class, Length: 84916, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save labels\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "labels = df['class']\n",
    "\n",
    "labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37652/2110503979.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_data = torch.load(\"text_softmaxes_test.pt\")\n",
      "/tmp/ipykernel_37652/2110503979.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  image_data = torch.load(\"image_softmaxes_test.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "text_data = torch.load(\"text_softmaxes_test.pt\")\n",
    "image_data = torch.load(\"image_softmaxes_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0369e-03, 3.8984e-04, 2.3128e-03, 7.8987e-04, 1.9691e-02, 5.9527e-04,\n",
       "        8.2609e-02, 7.3904e-01, 1.5808e-02, 1.4739e-02, 2.3738e-03, 1.6038e-02,\n",
       "        3.0135e-02, 2.4691e-04, 2.3044e-04, 4.5885e-05, 3.6524e-07, 6.7587e-02,\n",
       "        8.6550e-04, 2.3893e-05, 5.1681e-04, 1.1930e-03, 1.4306e-04, 1.6683e-03,\n",
       "        2.5537e-04, 2.1591e-04, 1.4530e-03])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in text_data:\n",
    "    assert len(t) == 27\n",
    "    assert t.sum().item() - 1 < 1e-6\n",
    "\n",
    "for t in image_data:\n",
    "    assert len(t) == 27\n",
    "    assert t.sum().item() - 1 < 1e-6\n",
    "\n",
    "assert len(text_data) == len(image_data)\n",
    "\n",
    "image_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Custom Dataset\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, text_data_path, image_data_path):\n",
    "        self.text_data = text_data.to(device)\n",
    "        self.image_data = image_data.to(device)\n",
    "        assert len(self.text_data) == len(self.image_data), \"Datasets must have same length\"\n",
    "        df = pd.read_csv(\"train.csv\")\n",
    "        # Label encode the classes to be in range [0, num_classes-1]\n",
    "        self.le = LabelEncoder()\n",
    "        encoded_labels = self.le.fit_transform(df[\"class\"].values)\n",
    "        # Convert encoded labels to tensor\n",
    "        self.labels = torch.tensor(encoded_labels, dtype=torch.long).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_tensor = self.text_data[idx]  # Already on device\n",
    "        image_tensor = self.image_data[idx]  # Already on device\n",
    "        combined_input = torch.cat([text_tensor, image_tensor])\n",
    "        label = self.labels[idx]\n",
    "        return combined_input, label\n",
    "\n",
    "# MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=54):  # 27 + 27 = 54 input features\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 27)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=device):\n",
    "    model = model.to(device)\n",
    "    best_val_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "        print(val_labels, val_preds)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation F1 Score: {val_f1:.4f}')\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), 'late_merger.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 10\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = CombinedDataset('text_softmaxes.pt', 'image_softmaxes.pt')\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = MLP()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train model\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37652/2348993138.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  late_merger.load_state_dict(torch.load('models/late_merger.pt'))\n"
     ]
    }
   ],
   "source": [
    "late_merger = MLP()\n",
    "late_merger.load_state_dict(torch.load('models/late_merger.pt'))\n",
    "\n",
    "dataset = CombinedDataset(\"text_softmaxes_test.pt\", \"image_softmaxes_test.pt\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "late_merger.eval()\n",
    "print(device)\n",
    "late_merger.to(device)\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels  in dataloader:\n",
    "        inputs, _ = inputs.to(device), labels.to(device)\n",
    "        outputs = late_merger(inputs)\n",
    "        best = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(best.cpu().numpy())\n",
    "\n",
    "len(all_preds)\n",
    "\n",
    "real_preds = dataset.le.inverse_transform(all_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('X_test.csv')\n",
    "res_df = pd.DataFrame({\"\": df_test['Unnamed: 0'], 'prdtypecode': real_preds})\n",
    "res_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
